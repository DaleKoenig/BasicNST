{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to test out using pretrained models and apply neural style transfer techniques. We will get the style layers from the VGG19 network since VGG is known to work well for style.  We will start by using VGG19 for content as well for simplicity, and perhaps change content networks later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is heavily influenced by the deeplearning.ai coursera specialization and the transfer learning tutorial provided by tensorflow.  Unlike the deeplearning.ai code, I will use keras, and precompute the style Gram matrices rather than computing them every cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the images used in testing this were taken fairly randomly from the internet, and have therefore not been included in the project (noteably 'table.jpeg' and 'beach.jpeg').  If running this code on a different computer, use any other images and change the file names in the code respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input, decode_predictions\n",
    "from scipy.misc import imsave\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import platform\n",
    "if(platform.system() == 'Darwin' and platform.release()[:2] == '17'):\n",
    "    print(\"This version of macOS crashes when using both matplotlib and keras, \\\n",
    "so we enable an os flag as a workaround.\")\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK']='True' # Workaround to remove crash for my macOS version\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets load the VGG19 network and try it out on a few examples.  If running this notebook elsewhere, it may be necessary to download a few random images and change the image names in this notebook appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagemodel = keras.applications.VGG19(weights='imagenet',include_top=True)\n",
    "# We must set each layer to be untrainable individually\n",
    "for layer in imagemodel.layers:\n",
    "    layer.trainable = False\n",
    "imagemodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define functions to load and preprocess an image, predict the label for an image, and to graph the top 5 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_image(imgpath):\n",
    "    \"\"\"Load the an image by its path and preprocess it to run through VGG\"\"\"\n",
    "    image = load_img(imgpath,target_size=(224,224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_guesses(image):\n",
    "    \"\"\"Return two lists containing the labels and probabilities for the most likely (up to 5) contents of an image\"\"\"\n",
    "    yhat = imagemodel.predict(image)\n",
    "    preds = decode_predictions(yhat)[0][:5] # Restrict to at most 5 predictions.\n",
    "    prediction_labels = [pred[1] for pred in preds]\n",
    "    prediction_probs = [pred[2] for pred in preds]\n",
    "    return prediction_labels, prediction_probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_predictions(prediction_labels, prediction_probs, subplotloc = 111):\n",
    "    \"\"\"Display a pyplot graph showing the top predictions and their probabilities for an image.\n",
    "    \n",
    "    Args:\n",
    "        prediction_names : a list of strings giving the names of the top predictions\n",
    "        prediction_probs : a list of probabilities (summing to at most 1) giving the associated probabilities\n",
    "        subplotloc : A tuple or integer to be passed to plt.subplot().  The default takes up the full figure.\n",
    "        \n",
    "        We assume 0 < len(prediction_names) == len(prediction_probs)\n",
    "    \"\"\"\n",
    "    pred_count = len(prediction_labels)\n",
    "    if(pred_count != len(prediction_probs)):\n",
    "        print(\"Error: Prediction label and probability vector have different lengths.\")\n",
    "        return\n",
    "    plt.subplot(subplotloc)\n",
    "    plt.bar(range(pred_count), prediction_probs)\n",
    "    _ = plt.xticks(range(pred_count), prediction_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test our functions by evaluating on a couple of sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels, prediction_probs = image_guesses(retrieve_image(\"table.jpeg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_predictions(prediction_labels, prediction_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_predictions(*image_guesses(retrieve_image(\"beach.jpeg\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!  Now lets prepare for the neural style transfer.  Initially we choose our style and content layers based off of the transfer learning tensorflow tutorial.  We can later update to optimize for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_image(image):\n",
    "    \"\"\"Adds noise to an image in VGG input array format.\"\"\"\n",
    "    \n",
    "    noise_mask = np.random.uniform(-20, 20, image.shape).astype('float32')\n",
    "    noisy_image = .5 * (image + noise_mask)\n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(processed_img):\n",
    "    \"\"\"Converts image in VGG input format to a normal image.\n",
    "    Code from Google's NST tensorflow tutorial.  See reference/license at the bottom.\n",
    "    \n",
    "    \"\"\"\n",
    "    x = processed_img.copy()\n",
    "    if len(x.shape) == 4:\n",
    "        x = np.squeeze(x, 0)\n",
    "    assert len(x.shape) == 3, (\"Input to deprocess image must be an image of \"\n",
    "                               \"dimension [1, height, width, channel] or [height, width, channel]\")\n",
    "    if len(x.shape) != 3:\n",
    "      raise ValueError(\"Invalid input to deprocessing image\")\n",
    "  \n",
    "    # perform the inverse of the preprocessiing step\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    x = x[:, :, ::-1]\n",
    "\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_content_cost(a_C, a_G):\n",
    "    \"\"\"\n",
    "    Computes the content cost\n",
    "    \n",
    "    Arguments:\n",
    "    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C \n",
    "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G\n",
    "    \n",
    "    Returns: \n",
    "    J_content -- content cost\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute the cost with tensorflow\n",
    "    # we do not bother scaling\n",
    "    J_content = tf.divide(tf.reduce_sum(tf.square(a_C-a_G)), 4. * tf.size(a_C, out_type=tf.float32) ** 2)\n",
    "    \n",
    "    return J_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(A):\n",
    "    \"\"\"Compute the Gram (autocorrelation) matrix of A.\n",
    "\n",
    "    Argument:\n",
    "    A -- matrix of shape (1, n_H, n_W, n_C)\n",
    "    \n",
    "    Returns:\n",
    "    GA -- Gram matrix of A, of shape (n_C, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape to [n_H*n_W, n_C]\n",
    "    n_C = A.get_shape()[-1]\n",
    "    A_unrolled = tf.reshape(A,[-1,n_C]) \n",
    "    # Compute Gram matrix\n",
    "    GA = tf.matmul(A_unrolled,A_unrolled,transpose_a=True)\n",
    "    return GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_style_cost(a_G, GS):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G\n",
    "    GS -- Gram matrix of the style image corresponding to the same layer\n",
    "    \n",
    "    Returns: \n",
    "    J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2)\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute Gram matrix for G\n",
    "    GG = gram_matrix(a_G)\n",
    "\n",
    "    # Computing the loss\n",
    "    J_style_layer = tf.divide(tf.reduce_sum(tf.square(GS-GG)),(4. * tf.size(GG, out_type=tf.float32) ** 2))\n",
    "    \n",
    "    return J_style_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_cost(model, image, content_outputs, style_gram_matrices, alpha = 10, beta = 40):\n",
    "    \"\"\"\n",
    "    Computes the total cost function\n",
    "    \n",
    "    Arguments:\n",
    "    model -- keras image model\n",
    "    image -- preprocessed image to apply model to\n",
    "    content_outputs -- outputs of the content layers from the content image\n",
    "    style_gram_matrices -- precomputed Gram matrices from the style outputs of the style image\n",
    "    alpha -- content loss weight\n",
    "    beta -- style loss weight\n",
    "    \n",
    "    Returns:\n",
    "    total cost -- weighted sum of the content and style costs\n",
    "    content_cost\n",
    "    style_cost\n",
    "    \"\"\"\n",
    "    outputs = model(image)\n",
    "    base_content = outputs[:len(content_outputs)]\n",
    "    base_style = outputs[len(content_outputs):]\n",
    "    \n",
    "    # Take average content and style cost over all content/style layers\n",
    "    content_cost = sum(compute_content_cost(a,b) for a,b in zip(base_content,content_outputs))\n",
    "    style_cost = sum(compute_layer_style_cost(a,b) for a,b in zip(base_style,style_gram_matrices))\n",
    "    total_cost = alpha*content_cost + beta*style_cost\n",
    "    \n",
    "    return total_cost, content_cost, style_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now precompute the style and content outputs of the style and content image respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = retrieve_image(\"ellie.JPG\")\n",
    "style_image = retrieve_image(\"picasso.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers = ['block5_conv2']\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1', \n",
    "                'block4_conv1', \n",
    "                'block5_conv1'\n",
    "               ]\n",
    "style_outputs = [imagemodel.get_layer(name).output for name in style_layers]\n",
    "content_outputs = [imagemodel.get_layer(name).output for name in content_layers]\n",
    "model_outputs = content_outputs + style_outputs\n",
    "model = keras.models.Model(inputs = imagemodel.inputs, outputs = model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NST(content_image, style_image, num_iterations, base_image = None):\n",
    "    \"\"\"Apply Neural style transfer to a randomized version of the content image to \n",
    "    produce an image in the style of the style image\n",
    "    \"\"\"\n",
    "    #content_image = retrieve_image(content_image_name)\n",
    "    #style_image = retrieve_image(style_image_name)\n",
    "    \n",
    "    content_outputs = model(content_image)[:len(content_layers)]\n",
    "    style_outputs = model(style_image)[len(content_layers):]\n",
    "    style_gram_matrices = [gram_matrix(A) for A in style_outputs]\n",
    "    if not base_image:\n",
    "        base_image = noisy_image(content_image)\n",
    "    base_image = tfe.Variable(base_image, dtype=tf.float32)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=5.)\n",
    "    \n",
    "    imgs = []\n",
    "        \n",
    "    for i in range(num_iterations):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, content_loss, style_loss = all_cost(model, base_image, content_outputs, style_gram_matrices)\n",
    "        grads = tape.gradient(loss, base_image)\n",
    "        opt.apply_gradients([(grads, base_image)])\n",
    "        \n",
    "        if i % 20 == 0: # Maybe increase if run on GPU but for now we can print every time.\n",
    "            imgs.append(deprocess_image(base_image.numpy()))\n",
    "            print('Iteration {}, loss {:.2f}, content loss {:.2f}, style loss {:.2f}'.format(i,loss, content_loss, style_loss))\n",
    "        else:\n",
    "            print('.',end='')\n",
    "    else:\n",
    "        #Final image\n",
    "        imgs.append(deprocess_image(base_image.numpy()))\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the neural style transfer and display the results.  The images are stored in the res variable every 20 iterations and can be saved to disk afterwards using, for example, scypi.misc.imsave(arr=res[i],name='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = NST(content_image, style_image, num_iterations=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(3,3, figsize=(10,10))\n",
    "for i, img in enumerate(res):\n",
    "    ax[i % 3, i // 3].imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I apply this to Ellie (my parents' dog) to get some neat images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='coloryellie.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portions of this page (specifically, the deprocess_image function) are reproduced from work created and shared by Google and licensed under the Apache 2.0 License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
